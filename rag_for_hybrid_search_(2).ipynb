{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erenarkangil/personalized_chatbot/blob/main/rag_for_hybrid_search_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61ea1c38-24ac-46ee-b8f5-fdeb65511131",
      "metadata": {
        "id": "61ea1c38-24ac-46ee-b8f5-fdeb65511131"
      },
      "source": [
        "# RAG Chat Bot for Hybrid Search\n",
        "\n",
        "This is the accompanying notebook for the [Oct 19 (2023) RAG for Hybrid Search meetup](https://www.pinecone.io/community/events/sf-meetup-october-2023/).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9960f94c-11a1-4c9b-99fc-0552e3f20886",
      "metadata": {
        "id": "9960f94c-11a1-4c9b-99fc-0552e3f20886"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/rag-for-hybrid/rag-for-hybrid-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/rag-for-hybrid/rag-for-hybrid-search.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f57977-9583-4772-86d9-e28e83dbe147",
      "metadata": {
        "id": "67f57977-9583-4772-86d9-e28e83dbe147"
      },
      "source": [
        "Quick notes:\n",
        "- You will need an OpenAI API Key\n",
        "- You will need a Pinecone account (API key & environment)\n",
        "- Cells that preview data are commented out, so that users can more easily navigate the notebook on Github. Run the notebook in Colab with these cells un-commented to see data previews.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I1Pm3RfT6hjO",
      "metadata": {
        "id": "I1Pm3RfT6hjO"
      },
      "outputs": [],
      "source": [
        "!pip3 install colab-xterm # Just makes the shell commands interactive, in case you have to press ENTER or type in 'Y/n' etc.\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3b8d11-6a04-4e7c-ace2-2bc28ab64b7f",
      "metadata": {
        "id": "bc3b8d11-6a04-4e7c-ace2-2bc28ab64b7f"
      },
      "outputs": [],
      "source": [
        "# Install libraries\n",
        "\n",
        "!pip install pymupdf\n",
        "!pip install faiss-cpu\n",
        "!pip install huggingface-hub==0.25.2\n",
        "\n",
        "#!pip install  pinecone-text==0.5.4\n",
        "!pip install  unstructured==0.10.24\n",
        "#!pip install  sentence-transformers==2.2.2\n",
        "!pip install  langchain==0.0.327\n",
        "!pip install  openai==0.28.1\n",
        "!pip install  pdfminer.six\n",
        "#!pip install  pdf2image==1.16.3\n",
        "!pip install python-dotenv==1.0.0\n",
        "#!pip install pytesseract==0.3.10\n",
        "#!pip install  unstructured_pytesseract==0.3.12\n",
        "#!pip install  huggingface-hub==0.20.2\n",
        "!pip install  numpy==2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "metadata": {
        "id": "RQg8Ncp8fC18"
      },
      "id": "RQg8Ncp8fC18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ikKvamd3TP7I"
      },
      "id": "ikKvamd3TP7I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show sentence-transformers"
      ],
      "metadata": {
        "id": "lUJRBInTQFoE"
      },
      "id": "lUJRBInTQFoE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5g-la06jXtsn",
      "metadata": {
        "id": "5g-la06jXtsn"
      },
      "outputs": [],
      "source": [
        "#!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6hrtPeeIa-I7",
      "metadata": {
        "id": "6hrtPeeIa-I7"
      },
      "outputs": [],
      "source": [
        "#sudo apt install tesseract-ocr\n",
        "#!sudo apt install libtesseract-dev"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6af345ef-6321-4a30-b2e3-9e84cbd4d5b2",
      "metadata": {
        "id": "6af345ef-6321-4a30-b2e3-9e84cbd4d5b2"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y sentence-transformers numpy\n",
        "#!pip install --no-cache-dir sentence-transformers"
      ],
      "metadata": {
        "id": "IGTPnsVAU1CV"
      },
      "id": "IGTPnsVAU1CV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0520a370-63a3-4f16-8cf1-acfc8cc7f914",
      "metadata": {
        "id": "0520a370-63a3-4f16-8cf1-acfc8cc7f914"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "import re\n",
        "from uuid import uuid4\n",
        "from typing import IO, Any, Dict, List, Tuple\n",
        "from copy import deepcopy\n",
        "import requests\n",
        "\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.documents.elements import Text\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "#from pinecone import Pinecone\n",
        "import openai\n",
        "from pinecone.core.client.model.query_response import QueryResponse\n",
        "\n",
        "from pinecone_text.sparse import BM25Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5858c70-9d3f-437d-a289-066ae69a64e2",
      "metadata": {
        "id": "c5858c70-9d3f-437d-a289-066ae69a64e2"
      },
      "source": [
        "Set up the environment variables we'll need. We recommend using `dotenv`. It's a super simple way to keep your variables safe, but accessible. Simply create a `.env` file with your secrets in it, and use the Python `dotenv` and `os` libraries to load them.\n",
        "\n",
        "To import your `.env` file into Colab, upload it (or create it) in the `/content/` dir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb61e9a-c191-489a-90e2-48df47e0c77b",
      "metadata": {
        "id": "8fb61e9a-c191-489a-90e2-48df47e0c77b"
      },
      "outputs": [],
      "source": [
        "%load_ext dotenv\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d62f04ff-5339-4097-b8e4-98a29450e879",
      "metadata": {
        "id": "d62f04ff-5339-4097-b8e4-98a29450e879"
      },
      "outputs": [],
      "source": [
        "# Make sure dotenv is in our kernel environment & working\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d6d9c5-2cf2-4796-8141-564e1b94994b",
      "metadata": {
        "id": "e0d6d9c5-2cf2-4796-8141-564e1b94994b"
      },
      "outputs": [],
      "source": [
        "pinecone_api_key = os.getenv('PINECONE_API_KEY')  # You can get your Pinecone api key and env (e.g. \"us-east-1\") at app.pinecone.io\n",
        "pinecone_env = os.getenv('PINECONE_ENV')\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "535ecd2e-fe6e-40fb-99bf-8b8333ef01fe",
      "metadata": {
        "id": "535ecd2e-fe6e-40fb-99bf-8b8333ef01fe"
      },
      "outputs": [],
      "source": [
        "# Let's make sure our dotenv secrets loaded correctly\n",
        "\n",
        "assert len(pinecone_api_key) > 0\n",
        "assert len(pinecone_env) > 0\n",
        "assert len(openai_api_key) > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acfaeac9-d63a-4d5a-b316-f714f032c421",
      "metadata": {
        "id": "acfaeac9-d63a-4d5a-b316-f714f032c421"
      },
      "source": [
        "# Download some articles we're interested in learning more about.\n",
        "\n",
        "Remember, hybrid search is best for knowledge that contains a lot of unique keywords that you'd like to search for, along with concepts you'd like clarity on, etc. Data that works best for this type of thing include medical data, most types of research data, data with lots of entities in it, etc.\n",
        "\n",
        "We'll be using Arxiv.org articles about different vector search algorithms for this demo. They've got lots of jargon and concepts that'll work great for hybrid search!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc05aea-08ac-493a-8bfe-6b0a90dbfb8f",
      "metadata": {
        "id": "9cc05aea-08ac-493a-8bfe-6b0a90dbfb8f"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "def get_pdf(base_url: str, filename: str):\n",
        "    \"\"\"\n",
        "    Download and write a PDF file from a github repository.\n",
        "\n",
        "    :param url: URL of Github repository containing the file you want to download & write locally.\n",
        "    \"\"\"\n",
        "    res = requests.get(base_url+filename)\n",
        "    # Check if the request was successful (HTTP status code 200)\n",
        "    if res.status_code == 200:\n",
        "      with open(filename, 'wb') as f:\n",
        "          f.write(res.content)\n",
        "          print(f\"PDF downloaded and saved as {filename}\")\n",
        "    else:\n",
        "      print(f\"Failed to download the PDF. HTTP status code: {res.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5f32da6-5ae9-47ac-a3d3-06bcdef1dfb7",
      "metadata": {
        "id": "c5f32da6-5ae9-47ac-a3d3-06bcdef1dfb7"
      },
      "outputs": [],
      "source": [
        "# Download our files to the /content/ dir in Colab\n",
        "\n",
        "github_dir = \"https://github.com/pinecone-io/examples/raw/master/learn/generation/rag-for-hybrid/\"\n",
        "filenames = [\"freshdiskann_paper.pdf\", \"hnsw_paper.pdf\", \"ivfpq_paper.pdf\"]\n",
        "\n",
        "for f in filenames:\n",
        "  get_pdf(github_dir, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tpgyK79h8ta0",
      "metadata": {
        "id": "tpgyK79h8ta0"
      },
      "outputs": [],
      "source": [
        "# Read in our file paths\n",
        "# Note: change this path to your local dir if running this notebook locally (i.e. not on Colab)\n",
        "\n",
        "freshdisk = os.path.join(\"/content/\", filenames[0])\n",
        "hnsw = os.path.join(\"/content/\", filenames[1])\n",
        "ivfpq = os.path.join(\"/content/\", filenames[2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e91a7b-2287-4200-b268-da25dec4f8c9",
      "metadata": {
        "id": "c8e91a7b-2287-4200-b268-da25dec4f8c9"
      },
      "source": [
        "# Partitioning & Cleaning our PDFs\n",
        "\n",
        "This step is optional. Partitioning simply uses ML to break a document up into pages, paragraphs, the title, etc. It's a nice-to-have that allows you to exclude certain elements you might not want to index, such as an article's bibliography (although we'll keep that since it could be useful information).\n",
        "\n",
        "If you want to skip this step, you can just read the PDFs into text or json, etc. and make your chunks straight from that object(s).\n",
        "\n",
        "Note: this notebook assumes you have partitioned your PDF. If you want to run this notebook from start to finish as-is, you'll need to run this step."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#nltk.download('punkt_tab')\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "RzvfLAfJVdBQ"
      },
      "id": "RzvfLAfJVdBQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fd90300-2c17-4b6d-bd12-624453c82fcc",
      "metadata": {
        "id": "2fd90300-2c17-4b6d-bd12-624453c82fcc"
      },
      "outputs": [],
      "source": [
        "# Let's partition all of our PDFs and store their partitions in a dictionary for easy retrieval & inspection later\n",
        "\n",
        "# Note: This takes a few mins to run (~12 mins; will be faster if running locally (~3 mins))\n",
        "\n",
        "partitioned_files = {\n",
        "    \"freshdisk\": partition_pdf(freshdisk, url=None, strategy = 'ocr_only'),\n",
        "    \"hnsw\": partition_pdf(hnsw, url=None, strategy = 'ocr_only'),\n",
        "    \"ivfpq\": partition_pdf(ivfpq, url=None, strategy = 'ocr_only'),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/partitioned_files.pkl', 'rb') as f:  # Adjust path if needed\n",
        "    partitioned_files = pickle.load(f)\n",
        "\n",
        "# Verify the loaded object\n",
        "type(partitioned_files), len(partitioned_files)  # Example check"
      ],
      "metadata": {
        "id": "H0ZIPOUBYPbJ"
      },
      "id": "H0ZIPOUBYPbJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qz2PaMHai5hN"
      },
      "id": "Qz2PaMHai5hN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SSotP27yjnd5"
      },
      "id": "SSotP27yjnd5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1cb1014-b774-42ba-8eb1-598762f3b434",
      "metadata": {
        "id": "b1cb1014-b774-42ba-8eb1-598762f3b434"
      },
      "outputs": [],
      "source": [
        "# Let's make an archived copy of partitioned_files dict so if we mess it up while cleaning, we don't have to re-ocr our PDFs:\n",
        "\n",
        "partitioned_files_copy = deepcopy(partitioned_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea7c9628-de78-45b1-946b-50e525e42f23",
      "metadata": {
        "id": "ea7c9628-de78-45b1-946b-50e525e42f23"
      },
      "outputs": [],
      "source": [
        "# partitioned_files.get('freshdisk')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1173112f-8449-4449-b520-2a19740fd940",
      "metadata": {
        "id": "1173112f-8449-4449-b520-2a19740fd940"
      },
      "source": [
        "You can see in the preview above that each of our PDFs now has elements classifying different parts of the text, such as `Text`, `Title`, and `EmailAddress`.\n",
        "\n",
        "Data cleaning matters a lot when it comes to hybrid search, because for the keyword-search part we care about each individual token (word).\n",
        "\n",
        "Let's filter out all of the email addresses to start with, since we don't need those for any reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0236621-8c64-453a-bb8b-44b7019c6671",
      "metadata": {
        "id": "a0236621-8c64-453a-bb8b-44b7019c6671",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def remove_unwanted_categories(elements: Dict[str, List[Text]], unwanted_cat: str) -> None:\n",
        "    \"\"\"\n",
        "    Remove partitions containing an unwanted category.\n",
        "\n",
        "    :parameter elements: Partitioned pieces of our documents.\n",
        "    :parameter unwanted_cat: The name of the category we'd like filtered out.\n",
        "    \"\"\"\n",
        "    for key, value in elements.items():\n",
        "        elements[key] = [i for i in value if not i.category == unwanted_cat]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00045ae3-4821-4200-a2fa-a3be94b216c1",
      "metadata": {
        "id": "00045ae3-4821-4200-a2fa-a3be94b216c1"
      },
      "outputs": [],
      "source": [
        "# Remove unwanted EmailAddress category from dictionary of partitioned PDFs\n",
        "\n",
        "remove_unwanted_categories(partitioned_files, 'EmailAddress')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9e35647-586a-4887-975e-bb86d091b7cd",
      "metadata": {
        "id": "f9e35647-586a-4887-975e-bb86d091b7cd"
      },
      "source": [
        "No more `EmailAddress` elements!:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "644d52c1-a0dd-4209-87c7-86f022630111",
      "metadata": {
        "id": "644d52c1-a0dd-4209-87c7-86f022630111"
      },
      "outputs": [],
      "source": [
        "# partitioned_files.get('freshdisk')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb46a39a-8dcd-46cd-bbfb-f755d1052754",
      "metadata": {
        "id": "bb46a39a-8dcd-46cd-bbfb-f755d1052754"
      },
      "source": [
        "To actually see what our elements are, we can call the `.text` attribute of each object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbabf1e0-656e-42e2-af4a-f6a3708e1cf2",
      "metadata": {
        "id": "dbabf1e0-656e-42e2-af4a-f6a3708e1cf2"
      },
      "outputs": [],
      "source": [
        "# Text preview of what's actually in one of our dictionary items:\n",
        "\n",
        "# [i.text for i in partitioned_files.get('freshdisk')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfade5a5-baa1-4479-8deb-8968cb8376ba",
      "metadata": {
        "id": "bfade5a5-baa1-4479-8deb-8968cb8376ba"
      },
      "source": [
        "You can see there are weird things like blank spaces, single letters, etc. as their own partitions. We don't want these either, so let's get rid of them.\n",
        "\n",
        "You can also see where some page breaks were that spanned single words -- these are identifiable by a word ending with a `- `. For these, we want to get rid of the `- ` and squish the word back together, so it makes sense.\n",
        "\n",
        "(You can also see that not all of the email addresses were caught by Unstructured's ML. It's too cumbersome to go through each doc and weed those out by hand, so we'll just have to leave them for now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22413a4e-1b61-41c6-83f3-1c7e3cc1ff54",
      "metadata": {
        "id": "22413a4e-1b61-41c6-83f3-1c7e3cc1ff54"
      },
      "outputs": [],
      "source": [
        "# Remove empty spaces & single-letter/-digit partitions:\n",
        "\n",
        "def remove_space_and_single_partitions(elements: Dict[str, List[Text]]) -> None:\n",
        "    \"\"\"\n",
        "    Remove empty partitions & partitions with lengths of 1.\n",
        "\n",
        "    :parameter elements: Partitioned pieces of our documents.\n",
        "    \"\"\"\n",
        "    for key, value in elements.items():\n",
        "        elements[key] = [i for i in value if len(i.text.strip()) > 1 ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e8aa65-d26e-4fc7-8f34-0ec2bc72da00",
      "metadata": {
        "id": "33e8aa65-d26e-4fc7-8f34-0ec2bc72da00"
      },
      "outputs": [],
      "source": [
        "remove_space_and_single_partitions(partitioned_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "562a2d57-7f94-4ff9-8ef0-96e5fa1ed7d2",
      "metadata": {
        "id": "562a2d57-7f94-4ff9-8ef0-96e5fa1ed7d2"
      },
      "source": [
        "No more single-character partitions or partitions with only whitespace, perfect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a7a897-8803-40eb-baaa-9dce221c03b3",
      "metadata": {
        "id": "b6a7a897-8803-40eb-baaa-9dce221c03b3"
      },
      "outputs": [],
      "source": [
        "# [i.text for i in partitioned_files.get('freshdisk')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c439c74-8a6b-4962-af7e-7e8d450e5b54",
      "metadata": {
        "id": "5c439c74-8a6b-4962-af7e-7e8d450e5b54"
      },
      "source": [
        "Let's now get rid of those strange words that have been split across page breaks (e.g. `funda- mental`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e62b963-64eb-4b03-a50c-7ae5d98e7197",
      "metadata": {
        "id": "7e62b963-64eb-4b03-a50c-7ae5d98e7197"
      },
      "outputs": [],
      "source": [
        "# Note: this function transforms our elemenets into their text representations\n",
        "\n",
        "def rejoin_split_words(elements: Dict[str, List[Text]]) -> None:\n",
        "    \"\"\"\n",
        "    Rejoing words that are split over pagebreaks.\n",
        "\n",
        "    :parameter elements: Partitioned pieces of our documents.\n",
        "    \"\"\"\n",
        "    for key, value in elements.items():\n",
        "        elements[key] = [i.text.replace('- ', '') for i in value if '- ' in i.text]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee954160-75f7-44b4-a5c1-5bda0037fcce",
      "metadata": {
        "id": "ee954160-75f7-44b4-a5c1-5bda0037fcce"
      },
      "outputs": [],
      "source": [
        "rejoin_split_words(partitioned_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eaf8942-ed2b-4053-9a51-55a678ed8909",
      "metadata": {
        "id": "1eaf8942-ed2b-4053-9a51-55a678ed8909"
      },
      "outputs": [],
      "source": [
        "# partitioned_files.get('freshdisk')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5325d394-1266-4292-bf46-1d2d41a798ad",
      "metadata": {
        "id": "5325d394-1266-4292-bf46-1d2d41a798ad"
      },
      "source": [
        "You can see now that we've sewn those split words back together:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "551b2690-d91f-44a6-9969-fe2cc2142cc5",
      "metadata": {
        "id": "551b2690-d91f-44a6-9969-fe2cc2142cc5"
      },
      "source": [
        "The last cleaning step we'll want to take is removing the inline citations, e.g. `[6, 9, 11, 16, 32, 35, 38, 43, 59]` and `[12]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6383895a-72ec-4923-aa00-f2848822d35e",
      "metadata": {
        "id": "6383895a-72ec-4923-aa00-f2848822d35e"
      },
      "outputs": [],
      "source": [
        "def remove_inline_citation_numbers(elements: Dict[str, List[Text]]) -> None:\n",
        "    \"\"\"\n",
        "    Remove inline citation numbers from partitions.\n",
        "\n",
        "    :parameter elements: Partitioned pieces of our documents.\n",
        "    \"\"\"\n",
        "    for key, value in elements.items():\n",
        "        pattern = re.compile(r'\\[\\s*(\\d+\\s*,\\s*)*\\d+\\s*\\]')\n",
        "        elements[key] = [pattern.sub('', i) for i in value]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c59afe18-b664-4a0a-8103-91049ecb9bca",
      "metadata": {
        "id": "c59afe18-b664-4a0a-8103-91049ecb9bca"
      },
      "outputs": [],
      "source": [
        "remove_inline_citation_numbers(partitioned_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17d99d35-605f-4dfc-af6f-47488aa2c7ab",
      "metadata": {
        "id": "17d99d35-605f-4dfc-af6f-47488aa2c7ab"
      },
      "source": [
        "We've still got some weird numbers in there, but it's pretty good!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f6cb1c5-ea66-4be9-9246-ae8a39e4ab28",
      "metadata": {
        "id": "8f6cb1c5-ea66-4be9-9246-ae8a39e4ab28"
      },
      "outputs": [],
      "source": [
        "# partitioned_files.get('freshdisk')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a653859f-4a0e-44b8-b49d-dc904cb141c9",
      "metadata": {
        "id": "a653859f-4a0e-44b8-b49d-dc904cb141c9"
      },
      "source": [
        "Now that we've cleaned our data, we can zip all the partitions (per PDF) back together so we're starting our chunking from a single, coherent text object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a8190b3-7d9d-45cd-8463-4e47729f9016",
      "metadata": {
        "id": "7a8190b3-7d9d-45cd-8463-4e47729f9016"
      },
      "outputs": [],
      "source": [
        "# Sew our partitions back together, per PDF:\n",
        "\n",
        "def stitch_partitions_back_together(elements: Dict[str, List[Text]]) -> None:\n",
        "    \"\"\"\n",
        "    Stitch partitions back into single string object.\n",
        "\n",
        "    :parameter elements:  Partitioned pieces of our documents.\n",
        "    \"\"\"\n",
        "    for key, value in elements.items():\n",
        "        elements[key] = ' '.join(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7051c57-483e-4262-92a1-84dfa00c3f6e",
      "metadata": {
        "id": "c7051c57-483e-4262-92a1-84dfa00c3f6e"
      },
      "outputs": [],
      "source": [
        "stitch_partitions_back_together(partitioned_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afc64d40-6205-4371-9b0b-a503b8fb45ef",
      "metadata": {
        "id": "afc64d40-6205-4371-9b0b-a503b8fb45ef"
      },
      "source": [
        "Good to go! All of our PDFs are now cleaned and single globs of text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a985d1a-caa0-4e91-a510-0b8b440f1a22",
      "metadata": {
        "id": "0a985d1a-caa0-4e91-a510-0b8b440f1a22",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "partitioned_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63780cff-5390-41a4-b53d-81772bcb800f",
      "metadata": {
        "id": "63780cff-5390-41a4-b53d-81772bcb800f"
      },
      "outputs": [],
      "source": [
        "# Let's save our cleaned files to a new variable that makes more sense w/the current state\n",
        "\n",
        "cleaned_files = partitioned_files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36bc4e5e-aa4d-42bc-9bb3-d47bb46a1a1e",
      "metadata": {
        "id": "36bc4e5e-aa4d-42bc-9bb3-d47bb46a1a1e"
      },
      "source": [
        "# Chunking our PDF content\n",
        "\n",
        "Chunking is integral to achieving great relevance with vector search, whether that's sparse vector search, dense vector search, or hybrid vector search.\n",
        "\n",
        "From our [chunking strategy post](https://www.pinecone.io/learn/chunking-strategies/):\n",
        "\n",
        "> The main reason for chunking is to ensure we’re embedding a piece of content with as little noise as possible that is still semantically relevant . . . For example, in semantic search, we index a corpus of documents, with each document containing valuable information on a specific topic. By applying an effective chunking strategy, we can ensure our search results accurately capture the essence of the user’s query. If our chunks are too small or too large, it may lead to imprecise search results or missed opportunities to surface relevant content. As a rule of thumb, if the chunk of text makes sense without the surrounding context to a human, it will make sense to the language model as well. Therefore, finding the optimal chunk size for the documents in the corpus is crucial to ensuring that the search results are accurate and relevant.\n",
        "\n",
        "We need to chunk our PDFs' (text) data into sizable chunks that are semantically coherent and dense with contextual information.\n",
        "\n",
        "We'll use LangChain's `RecusiveCharacterTextSplitter` since it's a super easy utility that makes chunking quick and customizable. You should experiment with different chunk sizes and overlap values to see how the resulting chunks differ. You want each chunk to make a reasonable amount of sense as a stand-alone data object. After some experimentation on our end, we will choose a `chunk_size` of `512` and a `chunk_overlap` of `35` (characters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1b8f24e-0052-41e7-9a23-8db8a6e829bb",
      "metadata": {
        "id": "d1b8f24e-0052-41e7-9a23-8db8a6e829bb"
      },
      "outputs": [],
      "source": [
        "def generate_chunks(doc: str, chunk_size: int = 512, chunk_overlap: int = 35) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Generate chunks of a certain size and token overlap.\n",
        "\n",
        "    :param doc: Document we want to turn into chunks.\n",
        "    :param chunk_size: Desired size of our chunks, in tokens (words).\n",
        "    :param chunk_overlap: Desired # of tokens (words) that will overlap across chunks.\n",
        "\n",
        "    :return: Chunks representations of the given document.\n",
        "    \"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = chunk_size,\n",
        "        chunk_overlap = chunk_overlap\n",
        "    )\n",
        "\n",
        "    return splitter.create_documents([doc])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "667392aa-ba7b-4f3a-8fb9-2b63e6066b95",
      "metadata": {
        "id": "667392aa-ba7b-4f3a-8fb9-2b63e6066b95"
      },
      "outputs": [],
      "source": [
        "def chunk_documents(docs: Dict[str, List[Text]],  chunk_size: int = 512, chunk_overlap: int = 35) -> None:\n",
        "    \"\"\"\n",
        "    Iterate over documents and chunk each one.\n",
        "\n",
        "    :parameter docs: The documents we want to chunk.\n",
        "    :param chunk_size: Desired size of our chunks, in tokens (words).\n",
        "    :param chunk_overlap: Desired # of tokens (words) that will overlap across chunks.\n",
        "    \"\"\"\n",
        "    for key, value in docs.items():\n",
        "        chunks = generate_chunks(value)\n",
        "        docs[key] = [c.page_content for c in chunks]  # Grab the text representation of the chunks via the `page_content` attribute\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a912cf2-4e14-47c5-a074-bcfd983056d9",
      "metadata": {
        "id": "1a912cf2-4e14-47c5-a074-bcfd983056d9"
      },
      "outputs": [],
      "source": [
        "chunk_documents(cleaned_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94320c93-1993-40ee-850e-687a71fe3da4",
      "metadata": {
        "id": "94320c93-1993-40ee-850e-687a71fe3da4"
      },
      "outputs": [],
      "source": [
        "chunked_files = cleaned_files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c646c92-3c4e-401a-a6e7-0481501ada96",
      "metadata": {
        "id": "5c646c92-3c4e-401a-a6e7-0481501ada96"
      },
      "source": [
        "Check out our chunks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ceb3744-e564-472d-a800-e50ee355681a",
      "metadata": {
        "id": "8ceb3744-e564-472d-a800-e50ee355681a"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e710a40-19b0-4957-89a6-30eaba4085d0",
      "metadata": {
        "id": "3e710a40-19b0-4957-89a6-30eaba4085d0"
      },
      "source": [
        "# Create Dense Embeddings of our Chunks\n",
        "\n",
        "Hybrid search needs both dense embeddings and sparse embeddings of the same content in order to work. Let's start with dense embeddings.\n",
        "\n",
        "We'll use the `'all-MiniLM-L12-v2'` [model](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2) hosted by HuggingFace to create our dense embeddings. It's currently high on their [MTEB (Massive Text Embedding Benchmark) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) (Reranking section), so it's a pretty safe bet. This will output dense vectors of 384 dimensions.\n",
        "\n",
        "Note: if you're playing around with this notebook, make sure to save your chunks and embeddings (both sparse and dense) in `pkl` [files](https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object), so that you don't have to wait for the embeddings to generate again if you want to rerun any steps in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "079ab428-d7d5-4aab-8ed3-e05408a5bfe3",
      "metadata": {
        "id": "079ab428-d7d5-4aab-8ed3-e05408a5bfe3"
      },
      "source": [
        "We'll have to create a dense embedding of each of our PDFs' chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3984fa37-32e3-43f3-9b97-b0013497a88e",
      "metadata": {
        "id": "3984fa37-32e3-43f3-9b97-b0013497a88e"
      },
      "outputs": [],
      "source": [
        "def produce_embeddings(chunks: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Produce dense embeddings for each chunk.\n",
        "\n",
        "    :param chunks: The chunks we want to create dense embeddings of.\n",
        "\n",
        "    :return: Dense embeddings produced by our SentenceTransformer model `all-MiniLM-L12-v2`.\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer('all-MiniLM-L12-v2')\n",
        "    embeddings = []\n",
        "    for c in chunks:\n",
        "        embedding = model.encode(c)\n",
        "        embeddings.append(embedding)\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "836ae697-df61-435f-b6da-093b0151e7b1",
      "metadata": {
        "id": "836ae697-df61-435f-b6da-093b0151e7b1"
      },
      "outputs": [],
      "source": [
        "freshdisk_dembeddings = produce_embeddings(chunked_files.get('freshdisk'))  # these take ~30s min to run"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(freshdisk_dembeddings)"
      ],
      "metadata": {
        "id": "bC8rmR2Pm8wJ"
      },
      "id": "bC8rmR2Pm8wJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(chunked_files.get('freshdisk'))"
      ],
      "metadata": {
        "id": "rhGX3XqihTag"
      },
      "id": "rhGX3XqihTag",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8bb67f-2b95-48f6-bbfa-ed3bd40a1574",
      "metadata": {
        "id": "7f8bb67f-2b95-48f6-bbfa-ed3bd40a1574"
      },
      "outputs": [],
      "source": [
        "hnsw_dembeddings = produce_embeddings(chunked_files.get('hnsw'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc992bc7-6c15-43e9-a88f-1f70412ea90c",
      "metadata": {
        "id": "fc992bc7-6c15-43e9-a88f-1f70412ea90c"
      },
      "outputs": [],
      "source": [
        "ivfpq_dembeddings = produce_embeddings(chunked_files.get('ivfpq'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fe90755-9203-45e1-a47b-51bedff337e1",
      "metadata": {
        "id": "1fe90755-9203-45e1-a47b-51bedff337e1"
      },
      "outputs": [],
      "source": [
        "# We can confirm the shape of each our dense embeddings is 384:\n",
        "\n",
        "# Make binary lists to keep track of any shapes that are *not* 384\n",
        "freshdisk_assertion = [0 for i in freshdisk_dembeddings if i.shape == 384]\n",
        "hnsw_assertion = [0 for i in hnsw_dembeddings if i.shape == 384]\n",
        "ivfpq_assertion = [0 for i in ivfpq_dembeddings if i.shape == 384]\n",
        "\n",
        "# Sum up our lists. If there are any embeddings that are not of shape 384, these sums will be > 0\n",
        "assert sum(freshdisk_assertion) == 0\n",
        "assert sum(hnsw_assertion) == 0\n",
        "assert sum(ivfpq_assertion) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cffb7427-6db0-42c4-ba3e-b589edadd3e2",
      "metadata": {
        "id": "cffb7427-6db0-42c4-ba3e-b589edadd3e2"
      },
      "source": [
        "# Create Sparse Embeddings of our Chunks\n",
        "\n",
        "Now we can create our sparse embeddings. We will use the BM25 algorithm to create our sparse embeddings. The resulting vector will represent an inverted index of the tokens in our chunks, constrained by things like chunk length.\n",
        "\n",
        "Pinecone has an awesome [text library](https://github.com/pinecone-io/pinecone-text) that makes generating these vectors super easy. We also have [a great notebook](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/search/semantic-search/sparse/bm25/bm25-vector-generation.ipynb) all about BM25 encodings.\n",
        "\n",
        "Since we're using a ML-implemented version of BM25, we need to \"fit\" the model to our corpus. To do this, we'll combine all 3 of our PDFs together, so that the BM25 model can compute all the token frequencies etc correctly. We'll then encode each of our documents with our \"fitted\" model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aa80a1a-bea3-4d49-90cc-33206e604899",
      "metadata": {
        "id": "2aa80a1a-bea3-4d49-90cc-33206e604899"
      },
      "outputs": [],
      "source": [
        "# Join the content of all our PDFs together into 1 large corpus\n",
        "\n",
        "corpus = \"\"\n",
        "\n",
        "for i, v in chunked_files.items():\n",
        "    corpus += ' '.join(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa6c477c-df4a-4b78-b0a6-421527650000",
      "metadata": {
        "id": "fa6c477c-df4a-4b78-b0a6-421527650000"
      },
      "outputs": [],
      "source": [
        "len(corpus)  # Awesome, we've got lots o' tokens here for our BM25 model to learn :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f124295c-9da5-4195-a57f-1a53f2feb842",
      "metadata": {
        "id": "f124295c-9da5-4195-a57f-1a53f2feb842"
      },
      "outputs": [],
      "source": [
        "# Initialize BM25 and fit to our corpus\n",
        "\n",
        "bm25 = BM25Encoder()\n",
        "bm25.fit(corpus)  # takes ~30s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b2ca25-2ac4-4712-bd8d-77607ef9c852",
      "metadata": {
        "id": "73b2ca25-2ac4-4712-bd8d-77607ef9c852"
      },
      "outputs": [],
      "source": [
        "# Create embeddings for each chunk\n",
        "freshdisk_sembeddings = [bm25.encode_documents(i) for i in chunked_files.get('freshdisk')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4b1c9f3-9192-4e46-afa2-6b4e93ebb6a0",
      "metadata": {
        "id": "e4b1c9f3-9192-4e46-afa2-6b4e93ebb6a0"
      },
      "outputs": [],
      "source": [
        "hnsw_sembeddings = [bm25.encode_documents(i) for i in chunked_files.get('hnsw')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87899d0e-2bcd-4fc5-8c47-f27672b8dcd9",
      "metadata": {
        "id": "87899d0e-2bcd-4fc5-8c47-f27672b8dcd9"
      },
      "outputs": [],
      "source": [
        "ivfpq_sembeddings = [bm25.encode_documents(i) for i in chunked_files.get('ivfpq')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f43b724-acdc-4c13-b321-5c63b4a3b907",
      "metadata": {
        "id": "5f43b724-acdc-4c13-b321-5c63b4a3b907"
      },
      "source": [
        "Let's look at the sparse embeddings for one of our PDFs.\n",
        "\n",
        "You'll see that each PDF's chunks has now transformed into a dictionary with `indices` and `values` keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a794f792-b1b5-458b-8f37-fd9e132cf907",
      "metadata": {
        "id": "a794f792-b1b5-458b-8f37-fd9e132cf907"
      },
      "outputs": [],
      "source": [
        "# freshdisk_sembeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02f495a3-37b8-4573-afca-3c8bfe6fb08f",
      "metadata": {
        "id": "02f495a3-37b8-4573-afca-3c8bfe6fb08f"
      },
      "outputs": [],
      "source": [
        "# We want the # of chunks per PDF to be equal to the # of sparse embeddings we've generated. Let's check that:\n",
        "\n",
        "assert len(freshdisk_sembeddings) == len(chunked_files.get('freshdisk'))\n",
        "assert len(hnsw_sembeddings) == len(chunked_files.get('hnsw'))\n",
        "assert len(ivfpq_sembeddings) == len(chunked_files.get('ivfpq'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunked_files.get('freshdisk')) +  len(chunked_files.get('hnsw')) + len(chunked_files.get('ivfpq'))"
      ],
      "metadata": {
        "id": "Tm1m8xdOPtou"
      },
      "id": "Tm1m8xdOPtou",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunked_files.get('freshdisk'))"
      ],
      "metadata": {
        "id": "iWWxdnARPt82"
      },
      "id": "iWWxdnARPt82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ece747e9-abc4-4289-aac2-b2fef0831339",
      "metadata": {
        "id": "ece747e9-abc4-4289-aac2-b2fef0831339"
      },
      "source": [
        "# Getting Our Embeddings into Pinecone\n",
        "\n",
        "Now that we have made our sparse and dense embeddings, it's time to index them into our Pinecone index.\n",
        "\n",
        "One thing to note is that only [p1 and s1 pods support hybrid search](https://docs.pinecone.io/docs/indexes). Since we're not concerned about high throughput for a demo, we'll go with s1, which is optimized for storage over throughput.\n",
        "\n",
        "Hybrid search indexes inherently also need `\"dotproduct\"` as their similarity `metric`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_ids(chunks: List[str]) -> List[str]:\n",
        "    \"\"\"Create unique IDs for each document chunk.\"\"\"\n",
        "    return [str(uuid4()) for _ in range(len(chunks))]\n",
        "\n",
        "# Generate unique IDs\n",
        "freshdisk_ids = create_ids(chunked_files.get('freshdisk'))\n",
        "hnsw_ids = create_ids(chunked_files.get('hnsw'))\n",
        "ivfpq_ids = create_ids(chunked_files.get('ivfpq'))"
      ],
      "metadata": {
        "id": "eNGMINElObPz"
      },
      "id": "eNGMINElObPz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Define the vector dimension (must match your embedding size)\n",
        "dimension = 384\n",
        "\n",
        "# Create a FAISS index\n",
        "f_index = faiss.IndexFlatL2(dimension)"
      ],
      "metadata": {
        "id": "723gg7qcObFN"
      },
      "id": "723gg7qcObFN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(freshdisk_dembeddings, dtype=np.float32)"
      ],
      "metadata": {
        "id": "lBU6Ec-2hNrV"
      },
      "id": "lBU6Ec-2hNrV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freshdisk_dembeddings_np = np.array(freshdisk_dembeddings, dtype=np.float32)\n",
        "hnsw_dembeddings_np = np.array(hnsw_dembeddings, dtype=np.float32)\n",
        "ivfpq_dembeddings_np = np.array(ivfpq_dembeddings, dtype=np.float32)\n",
        "\n",
        "# Add vectors to the FAISS index\n",
        "#index.add(freshdisk_dembeddings_np)\n",
        "#index.add(hnsw_dembeddings_np)\n",
        "#index.add(ivfpq_dembeddings_np)\n",
        "\n",
        "# Verify number of vectors added\n",
        "#print(\"Total vectors in index:\", index.ntotal)"
      ],
      "metadata": {
        "id": "WD-1GyVhOa4J"
      },
      "id": "WD-1GyVhOa4J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_faiss(query_embedding, k=5):\n",
        "    \"\"\"\n",
        "    Perform a similarity search in the FAISS index.\n",
        "\n",
        "    :param query_embedding: The embedding of the query text.\n",
        "    :param k: Number of closest matches to return.\n",
        "\n",
        "    :return: List of top-k indices and distances.\n",
        "    \"\"\"\n",
        "    query_embedding_np = np.array(query_embedding, dtype=np.float32).reshape(1, -1)\n",
        "    distances, indices = f_index.search(query_embedding_np, k)\n",
        "    return indices[0], distances[0]\n",
        "\n",
        "# Example query\n",
        "query_embedding = freshdisk_dembeddings[0]  # Use one of your embeddings as a test query\n",
        "indices, distances = search_faiss(query_embedding)\n",
        "\n",
        "print(\"Top matches:\", indices)\n",
        "print(\"Distances:\", distances)"
      ],
      "metadata": {
        "id": "ooIYeJP0OagO"
      },
      "id": "ooIYeJP0OagO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9bb3e59a-2656-457f-aa88-e54fa60b2a63",
      "metadata": {
        "id": "9bb3e59a-2656-457f-aa88-e54fa60b2a63"
      },
      "source": [
        "We'll create an index object out of the index we just made. We'll make this with Pinecone's [GRPC client](https://docs.pinecone.io/docs/performance-tuning#using-the-grpc-client-to-get-higher-upsert-speeds), since it's a little faster for upserts:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89dc6979-bd52-4080-b7cc-80689b6daf91",
      "metadata": {
        "id": "89dc6979-bd52-4080-b7cc-80689b6daf91"
      },
      "source": [
        "We'll need to make unique IDs for all of our objects, which is easy with the `uuid` library in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46051402-3950-4c6c-a0ab-ac6b3a08e389",
      "metadata": {
        "id": "46051402-3950-4c6c-a0ab-ac6b3a08e389"
      },
      "outputs": [],
      "source": [
        "# Let's make sure we have the same # of IDs as there are chunks:\n",
        "\n",
        "assert len(freshdisk_ids) == len(chunked_files.get('freshdisk'))\n",
        "assert len(hnsw_ids) == len(chunked_files.get('hnsw'))\n",
        "assert len(ivfpq_ids) == len(chunked_files.get('ivfpq'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eef3d9c-94f0-40d6-8c6c-7fef149c4152",
      "metadata": {
        "id": "7eef3d9c-94f0-40d6-8c6c-7fef149c4152"
      },
      "source": [
        "Now that we have our IDs, we can make our composite sparse-dense objects that we'll index into Pinecone. These will take 4 components:\n",
        "- Our IDs\n",
        "- Our sparse embeddings\n",
        "- Our dense embeddings\n",
        "- Our chunks\n",
        "\n",
        "We'll use the actual text content of our PDFs (stored in our chunks) as metadata. This allows the end user to see the content of what's being returned by their search instead of just the sparse/dense vectors. In order to store our chunks' textual data in digestible metadata object for Pinecone, we'll want to turn each chunk into a dict that has a `'text'` key to hold the chunk value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f994d0a-6167-4157-8e7f-0e450d164197",
      "metadata": {
        "id": "9f994d0a-6167-4157-8e7f-0e450d164197"
      },
      "outputs": [],
      "source": [
        "def create_metadata_objs(doc: List[str]) -> List[dict[str]]:\n",
        "    \"\"\"\n",
        "    Create objects to store as metadata alongside our sparse and dense vectors in our hybird Pinecone index.\n",
        "\n",
        "    :param doc: Chunks of a document we'd like to use while creating metadata objects.\n",
        "\n",
        "    :return: Metadata objects with a \"text\" key and a value that points to the text content of each chunk.\n",
        "    \"\"\"\n",
        "    return [{'text': d} for d in doc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "874f6265-5c21-4169-a8b9-2bd65f5b4a57",
      "metadata": {
        "id": "874f6265-5c21-4169-a8b9-2bd65f5b4a57"
      },
      "outputs": [],
      "source": [
        "freshdisk_metadata = create_metadata_objs(chunked_files.get('freshdisk'))\n",
        "hnsw_metadata = create_metadata_objs(chunked_files.get('hnsw'))\n",
        "ivfpq_metadata = create_metadata_objs(chunked_files.get('ivfpq'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6a6a557-49cc-4dbf-9c00-f02c447c4c48",
      "metadata": {
        "id": "f6a6a557-49cc-4dbf-9c00-f02c447c4c48"
      },
      "outputs": [],
      "source": [
        "# Preview\n",
        "\n",
        "freshdisk_metadata[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e2add63-efaa-4022-bf18-1472ae8aacee",
      "metadata": {
        "id": "9e2add63-efaa-4022-bf18-1472ae8aacee"
      },
      "outputs": [],
      "source": [
        "def create_composite_objs(ids: str, sembeddings: List[Dict[str, List[Any]]], dembeddings: List[float], metadata: Dict[str, str]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Create objects for indexing into Pinecone. Each object contains a document ID (which corresponds to the chunk, not the larger document),\n",
        "    the chunk's sparse embedding, the chunk's dense embedding, and the chunk's corresponding metadata object.\n",
        "\n",
        "    :param ids: Unique ID of a chunk we want to index.\n",
        "    :param sembeddings: Sparse embedding representation of a chunk we want to index.\n",
        "    :param dembeddings: Dense embedding representation of a chunk we want to index.\n",
        "    :param metadata: Metadata objects with a \"text\" key and a value that points to the text content of each chunk.\n",
        "\n",
        "    :return: Composite objects in the correct format for ingest into Pinecone.\n",
        "    \"\"\"\n",
        "    to_index = []\n",
        "\n",
        "    for i in range(len(metadata)):\n",
        "        to_index_obj = {\n",
        "                'id': ids[i],\n",
        "                'sparse_values': sembeddings[i],\n",
        "                'values': dembeddings[i],\n",
        "                'metadata': metadata[i]\n",
        "            }\n",
        "        to_index.append(to_index_obj)\n",
        "    return to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f44a46-a6b8-4778-bab7-429e8733c181",
      "metadata": {
        "id": "57f44a46-a6b8-4778-bab7-429e8733c181"
      },
      "outputs": [],
      "source": [
        "freshdisk_com_objs = create_composite_objs(freshdisk_ids, freshdisk_sembeddings, freshdisk_dembeddings, freshdisk_metadata)\n",
        "hnsw_com_objs = create_composite_objs(hnsw_ids, hnsw_sembeddings, hnsw_dembeddings, hnsw_metadata)\n",
        "ivfpq_com_objs = create_composite_objs(ivfpq_ids, ivfpq_sembeddings, ivfpq_dembeddings, ivfpq_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938a4954-ecf5-4491-87e6-597a83956024",
      "metadata": {
        "id": "938a4954-ecf5-4491-87e6-597a83956024"
      },
      "outputs": [],
      "source": [
        "len(freshdisk_dembeddings_np) + len(hnsw_dembeddings_np) + len(ivfpq_dembeddings_np)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8hl26ljNRG97"
      },
      "id": "8hl26ljNRG97",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tIdEz25XRHje"
      },
      "id": "tIdEz25XRHje",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "40D_5SB7RHZL"
      },
      "id": "40D_5SB7RHZL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "db7bc28e-5ead-4d44-a2af-287ee3efd712",
      "metadata": {
        "id": "db7bc28e-5ead-4d44-a2af-287ee3efd712"
      },
      "source": [
        "Now we can index (\"upsert\") our objects into our Pinecone index!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aaaaf91-2353-430b-aec6-0e843c499bd7",
      "metadata": {
        "id": "6aaaaf91-2353-430b-aec6-0e843c499bd7"
      },
      "outputs": [],
      "source": [
        "f_index.add(freshdisk_dembeddings_np)\n",
        "f_index.add(hnsw_dembeddings_np)\n",
        "f_index.add(ivfpq_dembeddings_np)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29feb6c2-e6ad-47c4-834a-91ba08a72927",
      "metadata": {
        "id": "29feb6c2-e6ad-47c4-834a-91ba08a72927"
      },
      "outputs": [],
      "source": [
        "# Woo we have our vectors (252) in our index!\n",
        "\n",
        "total_vectors = f_index.ntotal\n",
        "print(f\"Total vectors in the index: {total_vectors}\")\n",
        "\n",
        "# Check the index type\n",
        "print(f\"Index type: {type(f_index)}\")\n",
        "\n",
        "# If your index is using a specific search algorithm (e.g., IVF or HNSW), you can access more information:\n",
        "if isinstance(index, faiss.IndexIVFFlat):\n",
        "    print(f\"Number of centroids: {f_index.nlist}\")\n",
        "elif isinstance(index, faiss.IndexHNSWFlat):\n",
        "    print(f\"Number of neighbors for HNSW: {f_index.efConstruction}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f16c883-9892-49a6-99b5-846b356b3258",
      "metadata": {
        "id": "4f16c883-9892-49a6-99b5-846b356b3258"
      },
      "source": [
        "# Query Our Hybrid Docs\n",
        "\n",
        "Now that we have all of our hybrid vector objects in our Pinecone index, we can issue some queries!\n",
        "\n",
        "Since issuing a query to a vector index requires the query to be vectorized in the same way as the objects in the index are vectorized (so they can match up in vector space), for hybrid queries we'll have to vectorize the query *twice*! Once as a sparse vector and once as a dense vector. We then send both of those vectors to Pinecone to get items back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3802bd10-1a66-4a9b-b5c6-56a2779f30d9",
      "metadata": {
        "id": "3802bd10-1a66-4a9b-b5c6-56a2779f30d9"
      },
      "outputs": [],
      "source": [
        "query = \"What is the responsibility of each BSP Manager?\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0303a36-1bb0-40d5-85a3-eed31d003cab",
      "metadata": {
        "id": "c0303a36-1bb0-40d5-85a3-eed31d003cab",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "Create sparse embedding from query\n",
        "\n",
        "Note: do *not* refit the bm25 model here. We want to keep the token frequencies etc from when we fit it to the text from our PDFs!\n",
        "\n",
        "You might be wondering how the model gets \"refit\" when the corpus changes, the answer is a little complicated, but essentially this is a special implementation of BM25 (which usually runs online) that has precomputed frequencies for English words, based off the MSMarco dataset. So, when you add new docs to the corpus, you don't have to \"refit\" the BM25 model, it just finds the word frequencies in the MSMarco dataset.\n",
        "\n",
        "More here: https://github.com/pinecone-io/pinecone-text/blob/main/pinecone_text/sparse/bm25_encoder.py#L255\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd44e1c5-5ccc-4e94-8247-d83f2a32f02a",
      "metadata": {
        "id": "fd44e1c5-5ccc-4e94-8247-d83f2a32f02a"
      },
      "outputs": [],
      "source": [
        "query_sembedding = bm25.encode_queries(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4644d3a3-d5e3-4e6e-8383-ccd214bd0eaf",
      "metadata": {
        "id": "4644d3a3-d5e3-4e6e-8383-ccd214bd0eaf"
      },
      "outputs": [],
      "source": [
        "# Cool! We can see there are only two values in here, because BM25 automatically removed stop word like \"what\" and \"is\"\n",
        "\n",
        "query_sembedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20e4ad1b-418b-4865-9995-45bb8a351efb",
      "metadata": {
        "id": "20e4ad1b-418b-4865-9995-45bb8a351efb",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create dense embedding\n",
        "query = \"What is the responsibility of each BSP Manager?\"\n",
        "\n",
        "query_dembedding = produce_embeddings([query])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c366ef74-e1cd-46d1-b7a2-e856acb6ce8a",
      "metadata": {
        "id": "c366ef74-e1cd-46d1-b7a2-e856acb6ce8a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "type(query_dembedding)\n",
        "#query_dembedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ebc12c-a6a2-4834-b6ec-749fd61f32ae",
      "metadata": {
        "id": "06ebc12c-a6a2-4834-b6ec-749fd61f32ae"
      },
      "source": [
        "Pinecone vector search has a cool user feature where you can weight the sparse vectors higher or lower (i.e. of more or less importance) than the dense vectors. This is controlled by the `alpha` parameter. An `alpha` of 0 means you're doing a totally keyword-based search (i.e. only over sparse vectors), while an `alpha` of 1 means you're doing a totally semantic search (i.e. only over dense vectors).\n",
        "\n",
        "Let's make a function that'll let us weight our vectors by alpha.\n",
        "\n",
        "(We'll also include `k`, which is the number of docs we want to retrieve)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b61142a-192e-4b73-87f1-eeb68e8c4998",
      "metadata": {
        "id": "6b61142a-192e-4b73-87f1-eeb68e8c4998"
      },
      "outputs": [],
      "source": [
        "'''# Integrate alpha and top-k\n",
        "\n",
        "def weight_by_alpha(sparse_embedding: Dict[str, List[Any]], dense_embedding: List[float], alpha: float) -> Tuple[Dict[str, List[Any]], List[float]]:\n",
        "    \"\"\"\n",
        "    Weight the values of our sparse and dense embeddings by the parameter alpha (0-1).\n",
        "\n",
        "    :param sparse_embedding: Sparse embedding representation of one of our documents (or chunks).\n",
        "    :param dense_embedding: Dense embedding representation of one of our documents (or chunks).\n",
        "    :param alpha: Weighting parameter between 0-1 that controls the impact of sparse or dense embeddings on the retrieval and ranking\n",
        "        of returned docs (chunks) in our index.\n",
        "\n",
        "    :return: Weighted sparse and dense embeddings for one of our documents (chunks).\n",
        "    \"\"\"\n",
        "    if alpha < 0 or alpha > 1:\n",
        "        raise ValueError(\"Alpha must be between 0 and 1\")\n",
        "    hsparse = {\n",
        "        'indices': sparse_embedding['indices'],\n",
        "        'values':  [v * (1 - alpha) for v in sparse_embedding['values']]\n",
        "    }\n",
        "    hdense = [v * alpha for v in dense_embedding]\n",
        "    return hsparse, hdense'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0c12e86-5a18-41cf-bce0-699df56fbc8c",
      "metadata": {
        "id": "f0c12e86-5a18-41cf-bce0-699df56fbc8c"
      },
      "source": [
        "Now let's make a function that'll query our Pinecone index while taking into account whatever `alpha` and `k` values we want to pass:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "787c9898-b1d8-4eea-a8ec-5b24215c2a27",
      "metadata": {
        "id": "787c9898-b1d8-4eea-a8ec-5b24215c2a27"
      },
      "outputs": [],
      "source": [
        "'''# Note this doesn't have any genAI in it yet\n",
        "\n",
        "\n",
        "def issue_hybrid_query(sparse_embedding: Dict[str, List[Any]], dense_embedding: List[float], alpha: float, top_k: int) -> QueryResponse:\n",
        "    \"\"\"\n",
        "    Send properly formatted hybrid search query to Pinecone index and get back `k` ranked results (ranked by dot product similarity, as\n",
        "        defined when we made our index).\n",
        "\n",
        "    :param sparse_embedding: Sparse embedding representation of one of our documents (or chunks).\n",
        "    :param dense_embedding: Dense embedding representation of one of our documents (or chunks).\n",
        "    :param alpha: Weighting parameter between 0-1 that controls the impact of sparse or dense embeddings on the retrieval and ranking\n",
        "        of returned docs (chunks) in our index.\n",
        "    :param top_k: The number of documents (chunks) we want back from Pinecone.\n",
        "\n",
        "    :return: QueryResponse object from Pinecone containing top-k results.\n",
        "    \"\"\"\n",
        "    scaled_sparse, scaled_dense = weight_by_alpha(sparse_embedding, dense_embedding, alpha)\n",
        "\n",
        "    result = index.query(\n",
        "        vector=scaled_dense,\n",
        "        sparse_vector=scaled_sparse,\n",
        "        top_k=top_k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "    return result'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def weight_by_alpha(sparse_embedding: Dict[str, List[Any]], dense_embedding: List[float], alpha: float) -> Tuple[Dict[str, List[Any]], List[float]]:\n",
        "    \"\"\"\n",
        "    Weight the values of our sparse and dense embeddings by the parameter alpha (0-1).\n",
        "\n",
        "    :param sparse_embedding: Sparse embedding representation of one of our documents (or chunks).\n",
        "    :param dense_embedding: Dense embedding representation of one of our documents (or chunks).\n",
        "    :param alpha: Weighting parameter between 0-1 that controls the impact of sparse or dense embeddings on the retrieval and ranking\n",
        "        of returned docs (chunks) in our index.\n",
        "\n",
        "    :return: Weighted sparse and dense embeddings for one of our documents (chunks).\n",
        "    \"\"\"\n",
        "    if alpha < 0 or alpha > 1:\n",
        "        raise ValueError(\"Alpha must be between 0 and 1\")\n",
        "    hsparse = {\n",
        "        'indices': sparse_embedding['indices'],\n",
        "        'values': [v * (1 - alpha) for v in sparse_embedding['values']]\n",
        "    }\n",
        "    hdense = [v * alpha for v in dense_embedding]\n",
        "    return hsparse, hdense\n",
        "\n",
        "\n",
        "def issue_hybrid_query(sparse_embedding: Dict[str, List[Any]], dense_embedding: List[float], alpha: float, top_k: int) -> List[Tuple[int, float]]:\n",
        "    \"\"\"\n",
        "    Send a hybrid query to the FAISS index and get back top-k ranked results.\n",
        "\n",
        "    :param sparse_embedding: Sparse embedding representation of one of our documents (or chunks).\n",
        "    :param dense_embedding: Dense embedding representation of one of our documents (or chunks).\n",
        "    :param alpha: Weighting parameter between 0-1 that controls the impact of sparse or dense embeddings on the retrieval and ranking\n",
        "        of returned docs (chunks) in our index.\n",
        "    :param top_k: The number of documents (chunks) we want back from FAISS.\n",
        "\n",
        "    :return: List of tuples where each tuple is (index, similarity score) for the top-k results.\n",
        "    \"\"\"\n",
        "    # Weight the sparse and dense embeddings by alpha\n",
        "    scaled_sparse, scaled_dense = weight_by_alpha(sparse_embedding, dense_embedding, alpha)\n",
        "\n",
        "    # Convert the weighted dense vector to a numpy array for FAISS\n",
        "    dense_vector = np.array(scaled_dense, dtype=np.float32).reshape(1, -1)  # Reshaped to match FAISS input shape\n",
        "\n",
        "    # Perform the FAISS search for the dense vector\n",
        "    D, I = f_index.search(dense_vector, top_k)  # D: distances (similarities), I: indices\n",
        "\n",
        "    # Combine the results with the weighted sparse embeddings (assuming sparse embeddings are handled separately)\n",
        "    # Here, we return the results from FAISS based on the dense vector query.\n",
        "    results = [(I[0][i], D[0][i]) for i in range(top_k)]\n",
        "    return results\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "vFWivI_FURf_"
      },
      "id": "vFWivI_FURf_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_faiss_index(faiss_index: faiss.Index, query_embedding: np.ndarray, top_k: int) -> list:\n",
        "    \"\"\"\n",
        "    Perform a query on a FAISS index to get the top-k most similar results.\n",
        "\n",
        "    :param faiss_index: The FAISS index.\n",
        "    :param query_embedding: The query embedding to search for (shape: (1, dimension)).\n",
        "    :param top_k: The number of results to return.\n",
        "    :return: A list of tuples containing (index, similarity_score) for the top-k results.\n",
        "    \"\"\"\n",
        "    # Ensure the query embedding is in the correct format\n",
        "    query_embedding = np.array(query_embedding, dtype=np.float32).reshape(1, -1)\n",
        "\n",
        "    # Perform the search to get the top-k results\n",
        "    distances, indices = f_index.search(query_embedding, top_k)\n",
        "\n",
        "    # Return the results as a list of (index, similarity_score) tuples\n",
        "    return list(zip(indices[0], distances[0]))\n",
        "\n",
        "\n",
        "\n",
        "results = query_faiss_index(f_index, query_dembedding, 5)\n"
      ],
      "metadata": {
        "id": "D2sxSbYMqDbk"
      },
      "id": "D2sxSbYMqDbk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "HYC1RhMuqatG"
      },
      "id": "HYC1RhMuqatG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_texts_1 = list(chunked_files.get('freshdisk')) + list(chunked_files.get('hnsw')) + list(chunked_files.get('ivfpq'))\n"
      ],
      "metadata": {
        "id": "JZiHQIlSUR56"
      },
      "id": "JZiHQIlSUR56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_texts_1[0])\n",
        "chunked_files.get('freshdisk')[0]"
      ],
      "metadata": {
        "id": "oeFl8EHnb16Q"
      },
      "id": "oeFl8EHnb16Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(f_index))\n",
        "print(f_index.d)\n",
        "print(f_index.ntotal)  # This shows the total number of vectors in the index\n"
      ],
      "metadata": {
        "id": "MAzINl5npXw4"
      },
      "id": "MAzINl5npXw4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "626bffe8-cce1-47b9-84d6-c827e9f5cf07",
      "metadata": {
        "id": "626bffe8-cce1-47b9-84d6-c827e9f5cf07"
      },
      "source": [
        "Let's issue a pure semantic search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cea14e5f-20dc-44a1-8793-17583e8689ef",
      "metadata": {
        "id": "cea14e5f-20dc-44a1-8793-17583e8689ef",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Note, for our dense embedding (`query_dembedding`), we need to grab the 1st value [0] since Pinecone expects a Numpy array when queried:\n",
        "\n",
        "issue_hybrid_query(query_sembedding, query_dembedding[0], 0.0, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d98e72fd-4ace-42dc-8bd2-d086bd0907f4",
      "metadata": {
        "id": "d98e72fd-4ace-42dc-8bd2-d086bd0907f4",
        "scrolled": true
      },
      "source": [
        "And now a pure keyword search. You can see how many more domain-specific words are in these results:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "623df78c-e1db-440a-97fb-25d97e6678f0",
      "metadata": {
        "id": "623df78c-e1db-440a-97fb-25d97e6678f0"
      },
      "source": [
        "You can see the differences above: when we issue a purely semantic search, our search results are about what the idea of \"nearest neighbors\" is; in our keyword search, the vast majority of our search results are just exact-word matches for the tokens \"nearest\" and \"neighbors\". Most of them are just citations from the HNSW article's bibliography!\n",
        "\n",
        "Can we get the best of both worlds? In an ideal world, my search results would both tell me \"about\" the concept of nearest neighbors and contain things like citations that I could read more about later.\n",
        "\n",
        "Let's see if we can get a combination of semantic and keyword search by toggling our `alpha` value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a19923ef-af08-41cb-976b-f93fd4e0148d",
      "metadata": {
        "id": "a19923ef-af08-41cb-976b-f93fd4e0148d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "issue_hybrid_query(query_sembedding, query_dembedding[0], 0.2, 5)  # closer to 1.0 = closer to pure keyword search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_from_faiss_indices(faiss_indices, chunked_files):\n",
        "    \"\"\"\n",
        "    Given FAISS indices, return the corresponding text from chunked files.\n",
        "\n",
        "    :param faiss_indices: List of FAISS indices returned from a search.\n",
        "    :param chunked_files: Dictionary containing chunked text files.\n",
        "    :param embedding_type: The embedding type to use ('freshdisk', 'hnsw', or 'ivfpq').\n",
        "\n",
        "    :return: List of tuples (index, corresponding_text)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Retrieve the chunked text corresponding to the embedding type\n",
        "    documents = all_texts\n",
        "\n",
        "    # Map FAISS indices to the corresponding text chunks\n",
        "    return [(faiss_index, documents[faiss_index]) for faiss_index in faiss_indices]\n",
        "\n",
        "# Example of FAISS indices returned (e.g., from index.search)\n",
        "faiss_indices = [0,19,8,72,0]  # Top 5 indices returned from FAISS search\n",
        "\n",
        "result = get_text_from_faiss_indices(faiss_indices, chunked_files)\n",
        "\n",
        "# Output the result\n",
        "for index, text in result:\n",
        "    print(f\"Index: {index}, Text: {text}\")"
      ],
      "metadata": {
        "id": "7JL9EyBAZXQd"
      },
      "id": "7JL9EyBAZXQd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8CVcIz2ZYD-"
      },
      "id": "R8CVcIz2ZYD-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6f0e73e1-4ae3-4237-91bc-cea84a810a72",
      "metadata": {
        "id": "6f0e73e1-4ae3-4237-91bc-cea84a810a72"
      },
      "source": [
        "Amazing! You can see that our first couple search results are not very different than our pure keyword search. But when you get further down the results list, you'll see that we get an equation we can use to calculate KNN. That's a bit more useful than #3 in our pure keyword search, which is a bibliography entry. That's likely because we have semantic search in there too -- Pinecone knows we want to know \"about\" KNN, so it fetches items with lots of domain-specific terms (keyword search), but also items that demonstrate the \"aboutness\" of KNN (semantic search).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95fec382-1f9d-4e23-abbe-9897a00d531c",
      "metadata": {
        "id": "95fec382-1f9d-4e23-abbe-9897a00d531c"
      },
      "source": [
        "# Let's take a closer look. For science!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b16ea1f-e70a-4c44-af0a-95666cd212d8",
      "metadata": {
        "id": "8b16ea1f-e70a-4c44-af0a-95666cd212d8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de66f2e-7a86-4711-837b-8dfd19785e27",
      "metadata": {
        "id": "6de66f2e-7a86-4711-837b-8dfd19785e27",
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9912ce6a-e3bf-48f1-9e51-65c8cc3d8691",
      "metadata": {
        "id": "9912ce6a-e3bf-48f1-9e51-65c8cc3d8691"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb6cb84e-21b9-4cd0-b1f1-06c077ea58c0",
      "metadata": {
        "id": "eb6cb84e-21b9-4cd0-b1f1-06c077ea58c0",
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6945cec-025c-4425-831e-0aaeb4a83cf6",
      "metadata": {
        "id": "f6945cec-025c-4425-831e-0aaeb4a83cf6",
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b268e1e0-d59b-4d40-a19d-f084a8b108b9",
      "metadata": {
        "id": "b268e1e0-d59b-4d40-a19d-f084a8b108b9"
      },
      "source": [
        "Above, you can see the subtle ranking differences across each search type. For the most part, `document 8` is the top documents, except in `hybrid_1`, `hybrid_2` and `semantic`. In those two search types, `document 10` is the top document.\n",
        "\n",
        "It's up to you and your stakeholders to find the ideal `alpha` for your use case(s).\n",
        "\n",
        "Directly, for our use case, it seems anything >= `alpha=0.3` gets us similar results, so the impact of `alpha` is most discernable between `0.0-0.3`.\n",
        "\n",
        "Cool!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bb6ece2-cb19-43c0-9ab5-e0c0c09b46a2",
      "metadata": {
        "editable": true,
        "id": "5bb6ece2-cb19-43c0-9ab5-e0c0c09b46a2",
        "tags": []
      },
      "source": [
        "# Incorporating GenAI\n",
        "\n",
        "Now, hybrid search is cool enough, but what if you don't want to spend time sifting through your index's search results? What if you just want a single answer to a query?\n",
        "\n",
        "That's where GenAI comes in.\n",
        "\n",
        "We will make a retrieval augmented generation (RAG) pipeline that will make this happen.\n",
        "\n",
        "Since large language models (LLMs) do not know a ton of specific information (they are trained on the general Internet), especially if the information is from PDFs that it would have to download to have access to (like what are in our index), we need to give it this information!\n",
        "\n",
        "We do this by first sending our query to our Pinecone index and grabbing some search  results. We then attach these search results to our original query and send *both* to the LLM. That way, the LLM both knows what we want to ask it & can pull from its general knowledge store *and* has a specialized knowledge store (our Pinecone search results so that it can get us extra specific information.\n",
        "\n",
        "Let's try it out:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_augmented_queries(faiss_indices, chunked_files, query):\n",
        "    \"\"\"\n",
        "    Given FAISS indices and chunked files, return the augmented queries in different formats.\n",
        "\n",
        "    :param faiss_indices: List of FAISS indices returned from a search.\n",
        "    :param chunked_files: Dictionary containing chunked text files.\n",
        "    :param query: The original query to augment with context.\n",
        "\n",
        "    :return: Tuple of hybrid, pure keyword, and pure semantic augmented queries.\n",
        "    \"\"\"\n",
        "    # Retrieve the chunked text corresponding to the FAISS indices\n",
        "    documents = chunked_files\n",
        "    context = [all_texts_1[faiss_index] for faiss_index in faiss_indices]\n",
        "\n",
        "    # Combine the context with the query in the format desired\n",
        "    hybrid_augmented_query = \"\\n\\n---\\n\\n\".join(context) + \"\\n\\n-----\\n\\n\" + query\n",
        "\n",
        "    # Return all augmented queries\n",
        "    return hybrid_augmented_query\n",
        "\n",
        "\n",
        "\n",
        "# Dynamically provided FAISS indices (these could come from a FAISS search result)\n",
        "faiss_indices = [0, 19, 8, 72, 0]\n",
        "\n",
        "\n",
        "# Generate augmented queries\n",
        "hybrid_augmented_query= generate_augmented_queries(faiss_indices, chunked_files, query)\n",
        "\n",
        "# Output the results\n",
        "print(\"Hybrid Augmented Query:\")\n",
        "print(hybrid_augmented_query)\n"
      ],
      "metadata": {
        "id": "zfieHXOIsOd3"
      },
      "id": "zfieHXOIsOd3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11788caa-ea17-4678-93de-871f74d3057f",
      "metadata": {
        "id": "11788caa-ea17-4678-93de-871f74d3057f"
      },
      "outputs": [],
      "source": [
        "'''# We are then going to combine this \"context\" with our original query in a format that our LLM likes:\n",
        "\n",
        "hybrid_augmented_query = \"\\n\\n---\\n\\n\".join(hybrid_context)+\"\\n\\n-----\\n\\n\"+query\n",
        "pure_keyword_augmented_query = \"\\n\\n---\\n\\n\".join(pure_keyword_context)+\"\\n\\n-----\\n\\n\"+query\n",
        "pure_semantic_augmented_query = \"\\n\\n---\\n\\n\".join(pure_keyword_context)+\"\\n\\n-----\\n\\n\"+query'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "705cdd94-39ad-493e-b55b-12516b20ea1e",
      "metadata": {
        "id": "705cdd94-39ad-493e-b55b-12516b20ea1e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(hybrid_augmented_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "384afe12-83bf-4c10-baf0-0241ecd8132f",
      "metadata": {
        "id": "384afe12-83bf-4c10-baf0-0241ecd8132f"
      },
      "outputs": [],
      "source": [
        "# We are then going to give our LLM some instructions for how to act:\n",
        "\n",
        "query = 'What is the responsibility of each BSP Manager?'\n",
        "\n",
        "primer = f\"\"\"You are Q&A bot. A highly intelligent system that answers\n",
        "user questions based on the information provided by the user above\n",
        "each question. If the information can not be found in the information\n",
        "provided by the user you truthfully say \"I don't know, sorry\".\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf840d48-be91-4dd3-b7bb-9d0556da0b80",
      "metadata": {
        "id": "cf840d48-be91-4dd3-b7bb-9d0556da0b80",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Now we query our LLM with our augmented query & our primer!\n",
        "\n",
        "# Our hybrid query:\n",
        "\n",
        "openai.api_key = 'yourkey'\n",
        "\n",
        "\n",
        "hybrid_res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": hybrid_augmented_query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "hybrid_res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "466f61d4-1896-4716-a8e4-b25a4eda97fe",
      "metadata": {
        "id": "466f61d4-1896-4716-a8e4-b25a4eda97fe"
      },
      "source": [
        "You can see subtle differences across the different results above. It's up to you and your stakeholders to figure out what type of search (semantic, keyword, hybrid) offers the most relevant information for your end users"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d28051b6-219a-44bf-aee7-5894d6af600b",
      "metadata": {
        "id": "d28051b6-219a-44bf-aee7-5894d6af600b"
      },
      "source": [
        "# What if we take our our Pinecone vectors altogether??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6837f020-3147-4014-b31b-f0b070ae9a69",
      "metadata": {
        "id": "6837f020-3147-4014-b31b-f0b070ae9a69",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get the response\n",
        "answer = hybrid_res['choices'][0]['message']['content']\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(query)\n",
        "\n",
        "print(answer[:89])  # First part of the answer\n",
        "print(answer[89:])  # Second part of the answer\n"
      ],
      "metadata": {
        "id": "QoeQONUIzUX0"
      },
      "id": "QoeQONUIzUX0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0mGqmZJrv5Tn"
      },
      "id": "0mGqmZJrv5Tn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is COAM?\"\n",
        "\n",
        "query_dembedding = produce_embeddings([query])\n",
        "results = query_faiss_index(f_index, query_dembedding, 5)"
      ],
      "metadata": {
        "id": "986xV7TEwOBg"
      },
      "id": "986xV7TEwOBg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "M11DfCi9wONG"
      },
      "id": "M11DfCi9wONG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2b6943af-c430-4a5c-acd7-2a55f53a8f87",
      "metadata": {
        "id": "2b6943af-c430-4a5c-acd7-2a55f53a8f87"
      },
      "source": [
        "We can see that RAG really does have a huge impact! Without our PDFs, ChatGPT doesn't know much helpful detail at all! Nor can it give us bibliographic data for articles we might want to look up later!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_texts_1[83]"
      ],
      "metadata": {
        "id": "YWsq-OrowafF"
      },
      "id": "YWsq-OrowafF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_indices = [83,10,67,22,52]\n",
        "\n",
        "\n",
        "# Generate augmented queries\n",
        "hybrid_augmented_query= generate_augmented_queries(faiss_indices, chunked_files, query)\n",
        "\n",
        "# Output the results\n",
        "print(\"Hybrid Augmented Query:\")\n",
        "print(hybrid_augmented_query)\n"
      ],
      "metadata": {
        "id": "LlamJ6KowvlI"
      },
      "id": "LlamJ6KowvlI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": hybrid_augmented_query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "hybrid_res\n",
        "# Get the response\n",
        "answer = hybrid_res['choices'][0]['message']['content']\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "h9aOX_U6w9Jn"
      },
      "id": "h9aOX_U6w9Jn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query)\n",
        "\n",
        "print(answer[:90])  # First part of the answer\n",
        "print(answer[90:])  # Second part of the answer\n"
      ],
      "metadata": {
        "id": "KTmaxtuSw9QM"
      },
      "id": "KTmaxtuSw9QM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TUJzZPehxGpr"
      },
      "id": "TUJzZPehxGpr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}